{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al7VxlS3h0SO"
      },
      "source": [
        "# Hardware For ML Class Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQeCrhHvDqMy"
      },
      "source": [
        "# Modeling Albiero\n",
        "To model Albiero, we divide the dot product kernel into several steps:\n",
        "- Input Conversion:\n",
        "    - Handles conversion from DE -> AE -> AO.\n",
        "    - Accounts for the losses/noises that occur along the way.\n",
        "- Weight Conversion:\n",
        "    - Handlers conversion from DE -> AE.\n",
        "    - Accounts for normalization to [-1, 1].\n",
        "- The Dot Product itself.\n",
        "    - Performs the AE/AO dot product.\n",
        "    - Handles the conversion from AO to AE in the PD.\n",
        "- The Output conversion.\n",
        "    - Handles conversion from AE to quantized DE.\n",
        "\n",
        "\n",
        "I don't know if this is the level of expected detail, but it's a good start to actually understand what the accelerator is doing.\n",
        "\n",
        "\n",
        "There are many things I am very unsure about.\n",
        "I have left them as `TODO(Ask)` in the code. We should ask about them in office hours.\n",
        "Feel to modify the code or add your own questions.\n",
        "\n",
        "Once we have clarified these points, we can just turn these classes into pytorch operations, and run the DNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6f9ka50Vkua"
      },
      "source": [
        "# Outline of Clarifications to Ask\n",
        "## General Questions\n",
        "1. Level of Detail:\n",
        "    - It seems impossible to capture the noise without a semi-detailed step by step computation.\n",
        "    - Is this an overkill? What's the alternative? Looks like the proposal above seems basically good to go.\n",
        "2. Parameter Values:\n",
        "    - The paper does not specify all the values (e.g., feedback resistance at the PD, or crosstalk noise in PLCU MRRs)\n",
        "        - Do you know where to find them? TBD.\n",
        "        - Or can they be derived from the provided ones (e.g., MRR crosstalk from $k^2$ and FSR)? Look below for cross-talk for specifics.\n",
        "        - Or can we assume some 'ideal' default (e.g., the feedback resistance that would allow loss-less computation). Yes we can assume the ideal to begin with.\n",
        "3. Losses:\n",
        "    - In addition to noise, there are also losses.\n",
        "    - Do we ignore them, or do we take them into account? Answer: we should ignore them and we can justify this by mentioning that the losses are predictable. If the losses are not predictable then maybe we should model them.\n",
        "4. Cross-talk?\n",
        "    - Cross talk seems input dependent, meaning that the amount of noise depends on surrounding values (meaning receptive fields that are multiplexed in the same waveguide).\n",
        "    - Should we derive cross-talk for micro-ring resonator? Answer: We should try and if not we might not do it. Cross-talk is important.\n",
        "5. Do we assume constants or make something parameterized? Yes. Do not just hard-code.\n",
        "    \n",
        "\n",
        "## Specific Questions\n",
        "### Input Conversion\n",
        "- I understand that quantized inputs are turned into voltages.\n",
        "    - With what precision? In what range? Just assume some sort of ideal if it's very much not defined from the paper (i.e. just don't model it).\n",
        "    - Like [0, 1.0]?\n",
        "- The voltage is then turned into an optical signal, after being multiplied by a 'gain' in (W/V).\n",
        "    - I can't find this value.\n",
        "    - I can assume defaults that match the output deconversion?\n",
        "- AWG (Arrayed Waveguide Grating) Crosstalk.\n",
        "    - This is given as a fixed value in the paper.\n",
        "    - Can we assume it?\n",
        "    - Isn't crosstalk input-dependent.\n",
        "\n",
        "### Weight Conversion\n",
        "- The paper expects weights to be in [-1, 1]. So I assume we have to manually scale down, then scale back up right?\n",
        "- What the weights become voltages, can we assume a perfect conversion?\n",
        "    - E.g., if the weight is $0.378934373$, the voltage can exactly match that.\n",
        "\n",
        "### Optical Dot Product\n",
        "- How to compute MRR cross-talk?\n",
        "    - We are given $k^2$ (cross-coupling factor) and FSR (free spectral range).\n",
        "    - It should input-dependent?\n",
        "- How to capture RIN (relative intensity noise)?\n",
        "    - The units we are given are decibels relative to the carrier per hertz (dBc/Hz)?\n",
        "        - The bandwidth (frequency?) is later given as 5GHz.\n",
        "- How to get the \"feedback resistance\"?\n",
        "    - Allows converting current to voltage.\n",
        "\n",
        "### Output Conversion\n",
        "- How do we map voltage back to integers.\n",
        "- Like:\n",
        "    - Can we assume some uniform mapping, from (V_min -> 0) and (V_max -> int_max).\n",
        "    - Are V_min and V_max fixed parameters, or do change input by input?\n",
        "        - I.e., does 1V always correspond to the same integer, is it relative to other voltage values in the output.\n",
        "- Same question about voltage precision.\n",
        "    - Can we assume perfect voltage precision, or is something lost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TskEih9wj_PO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import typing as t\n",
        "import math\n",
        "import numpy as np\n",
        "from kernels import * \n",
        "from configurations import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-121.,  134.,   71.,    8., -184.,  -77.,   80., -233., -197.])\n",
            "tensor([ 1.0000,  0.3976,  0.7766, -0.4723, -0.4684, -0.7250, -0.0781,  0.4879,\n",
            "        -0.9299])\n",
            "Reference result:  tensor(188.9058)\n",
            "Final output:  tensor([191.8824])\n"
          ]
        }
      ],
      "source": [
        "# Regular dot product.\n",
        "seed = 47\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "input_tensor = torch.randint(-256, 256, (9,), dtype=torch.float)\n",
        "print(input_tensor)\n",
        "weight_tensor = (torch.rand((9,), dtype=torch.float) - 0.5) * 2\n",
        "weight_tensor[0] = 1 # For max to be 1\n",
        "print(weight_tensor)\n",
        "\n",
        "reference_result = torch.dot(input_tensor, weight_tensor)\n",
        "\n",
        "optical_dot_product = OpticalDotProduct(\n",
        "    weight_tensor,\n",
        "    OpticalDotProductConfiguration()\n",
        ")\n",
        "print(\"Reference result: \", reference_result)\n",
        "\n",
        "seed = torch.seed()&(2**32-1)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "print(\"Final output: \", optical_dot_product(input_tensor.unsqueeze(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[  553.9249,  -360.9922, -1071.8702,   118.0041],\n",
            "          [ -285.1765,   930.5353,  -216.3408,  -776.8600],\n",
            "          [-1602.8884,   -29.6167,   295.0101,  -329.1002],\n",
            "          [  540.8519,  -740.4177,  -767.0264,   648.5211]],\n",
            "\n",
            "         [[  546.9728,   804.1028,   405.2297,  -770.9247],\n",
            "          [ -602.9027,   157.5183,   375.5787,  -454.6479],\n",
            "          [   39.5346,  -992.2410,   385.4624,  -389.1445],\n",
            "          [  108.7202,  -843.4048,  -474.4152,  -642.0884]],\n",
            "\n",
            "         [[   57.4689,  1125.0908,  -474.9167,   251.9966],\n",
            "          [  271.3810,  -318.5876,   310.1497,    48.4609],\n",
            "          [  407.0715,   525.4301,   581.5306,   171.7226],\n",
            "          [ -116.3061,   690.8433,   552.4542,  -314.8247]]]])\n",
            "////\n",
            "tensor([[[[  557.7761,  -362.6187, -1072.8365,   116.1932],\n",
            "          [ -286.9122,   927.6728,  -218.5527,  -780.7416],\n",
            "          [-1601.7513,   -26.1975,   290.2320,  -330.9620],\n",
            "          [  535.1952,  -743.8896,  -764.5191,   648.2355]],\n",
            "\n",
            "         [[  548.5717,   808.9258,   410.4953,  -766.2744],\n",
            "          [ -600.6193,   157.9940,   374.1320,  -450.6280],\n",
            "          [   42.7624,  -998.5942,   381.9414,  -389.5406],\n",
            "          [  112.1237,  -841.7892,  -477.6503,  -638.5294]],\n",
            "\n",
            "         [[   55.2295,  1121.3198,  -477.9128,   258.0916],\n",
            "          [  266.4760,  -319.1744,   306.7489,    43.5325],\n",
            "          [  404.8857,   523.2145,   576.4692,   173.3023],\n",
            "          [ -118.8609,   692.3616,   548.9272,  -308.7210]]]])\n"
          ]
        }
      ],
      "source": [
        "# Regular convolution.\n",
        "#seed = 49\n",
        "#torch.manual_seed(seed)\n",
        "#np.random.seed(seed)\n",
        "input_tensor = torch.randint(-256, 256, (1,5,9,9), dtype=torch.float)\n",
        "weight_tensor = (torch.rand((3,5,3,3), dtype=torch.float) - 0.5) * 2\n",
        "\n",
        "\n",
        "#print(input_tensor)\n",
        "#print(weight_tensor)\n",
        "\n",
        "conv = OpticalConvolution(weight_tensor, OpticalDotProductConfiguration(), None, stride=2)\n",
        "print(conv(input_tensor))\n",
        "print(\"////\")\n",
        "print(F.conv2d(input_tensor, weight_tensor, stride=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-590.7535,   28.2648,  432.7520],\n",
            "        [  22.6110, -211.5579,  469.8863]])\n",
            "////\n",
            "tensor([[-590.4590,   31.0448,  430.2338],\n",
            "        [  20.9548, -208.3773,  471.8492]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#seed = 48\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "batch_size=2\n",
        "N=16\n",
        "O=3\n",
        "input_tensor = torch.randint(-256, 256, (batch_size, N), dtype=torch.float)\n",
        "weight_tensor = (torch.rand((O, N), dtype=torch.float) - 0.5) * 2\n",
        "bias_tensor = (torch.rand((O,), dtype=torch.float) - 0.5) * 2\n",
        "\n",
        "# PyTorch FCC\n",
        "linear = torch.nn.Linear(N, O)\n",
        "linear.weight.data = weight_tensor.clone()\n",
        "linear.bias.data = bias_tensor.clone()\n",
        "\n",
        "fc = OpticalFC(weight_tensor, bias_tensor, OpticalDotProductConfiguration())\n",
        "print(fc(input_tensor))\n",
        "print(\"////\")\n",
        "print(linear(input_tensor))  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
